---
title: "Introduction: Integration of scRNA-seq data and evaluation"
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
vignette: >
  %\VignetteIndexEntry{Introduction: Integration of scRNA-seq data and evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{css, echo=FALSE}
.darker-code {
  color: #2f2f2f;
}
```

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  results = "hold",
  message = FALSE,
  class.source  = "darker-code"
)
```

```{r setup}
library(SeuratIntegrate)
```

## Install and load the data

We will use the `pbmcsca` dataset available from the package `SeuratData`.
```{r install, eval=FALSE}
# install `SeuratData` package (if not yet)
if (! requireNamespace("SeuratData", quietly = TRUE)) {
  devtools::install_github('satijalab/seurat-data')
}

# increase download timeout and install the dataset
options(timeout = 300)
SeuratData::InstallData('pbmcsca')
```

```{r data}
# load the dataset (take 1,000 first cells to speed-up execution)
seu <- SeuratData::LoadData('pbmcsca')[,1:1e3]
```

## Inspect the dataset

Have a look at the metadata:

```{r}
# rmarkdown::paged_table -> prints data frames in a less ugly way than default
rmarkdown::paged_table(head(seu[[]], n = 10))
```

The `Method` column seems to contain the cells' batch of origin.

```{r}
table(seu$Method)
```

There's also a `CellType` column that contains cell-types estimates that we will
use later on.

```{r}
rmarkdown::paged_table(as.data.frame(table(seu$CellType)))
```

Let's now define the batch variable and the cell-type variable(s):

```{r batch-cell-vars}
batch.var <- 'Method'   # available in the metadata of the object
cell.var <- 'CellType'  # available in the metadata of the object
```

## Process the data in a standard fashion

For now, our Seurat object is not split. Each layer contains cells from all
batches.
```{r}
cat('Layers before split:', paste(Layers(seu), collapse = ", "), '\n')
```


We pick the `Method` column and separate the cell batches accordingly.
This way, we obtain one layer per batch.
```{r split-batch}
seu[['RNA']] <- split(seu[['RNA']], f = seu$Method)

cat('Layers after split:', paste(Layers(seu), collapse = ", "), '\n')
```

Then, we proceed to the standard `Seurat` workflow until we obtain the PCA reduction.

```{r, echo=FALSE}
knitr::asis_output('<a id="standard-worflow"></a>')
```
```{r standard-worflow, results='hide', warning=FALSE}
seu <- SCTransform(seu)
seu <- RunPCA(seu, verbose = F)
seu <- FindNeighbors(seu, reduction = "pca", dims = 1:30, k.param = 20L)
```

```{r umap, warning=FALSE}
seu <- FindClusters(seu, graph.name = 'SCT_snn', resolution = .5)
seu <- RunUMAP(seu, dims = 1:30, reduction = 'pca')
```

```{r, fig.height=5}
DimPlot(seu, label = T) + NoLegend() + ggplot2::coord_fixed(ratio = .7)
```

```{r, fig.width=10, fig.height=4}
DimPlot(seu, group.by = batch.var) +
  DimPlot(seu, group.by = cell.var) & ggplot2::coord_fixed(ratio = .7)
```

## Integrate batches

The integration commands are to some extent similar to the
[Seurat V5 vignette](https://satijalab.org/seurat/articles/seurat5_integration).
The purpose of this package is to extend the set of available integration methods.
See the bottom of `?IntegrateLayers` to have a comprehensive list of relevant
methods.

Many methods are implemented in python, and the wrappers developed for this
package rely on the `reticulate` package and `conda` environments. If you are
not familiar with the `CondaEnvManager`, have a look at the vignette
**setup_and_tips**.

Let's perform a few integrations. To this purpose, one might:

-   use the function `DoIntegrate()` from SeuratIntegrate with integration
methods
-   check the method-specific arguments (e.g. `?bbknnIntegration`)
-   first set up the `CondaEnvManager` before using python-based methods (check
the vignette **setup_and_tips**)
-   when a method requires a `conda` environment, the conda environment from the
manager is called by default (`conda_env = NULL`) . Inputting a path to a conda
environment or its name overrides the default behaviour by loading the specified
environment.

Some methods expect a raw count matrix, others expect the scaled counts of the
variable features, etc. To help you with the choice, look at the table below

+---------------+-------------------------+-------------------------------------------------------------------+
| Method        | Layer                   | Features                                                          |
+===============+=========================+===================================================================+
| ComBat        | data                    | any but better with restricted number of features (e.g. variable) |
+---------------+-------------------------+-------------------------------------------------------------------+
| Harmony       | N/A (PCA reduction)     | N/A (PCA reduction)                                               |
+---------------+-------------------------+-------------------------------------------------------------------+
| MNN           | data                    | any but better with restricted number of features (e.g. variable) |
+---------------+-------------------------+-------------------------------------------------------------------+
| bbknn         | N/A (PCA reduction)     | N/A (PCA reduction)                                               |
+---------------+-------------------------+-------------------------------------------------------------------+
| scVI / scANVI | counts                  | all                                                               |
+---------------+-------------------------+-------------------------------------------------------------------+
| Scanorama     | counts or data          | any                                                               |
+---------------+-------------------------+-------------------------------------------------------------------+
|               | data (recon loss "mse") | all                                                               |
| trVAE         |                         |                                                                   |
|               | counts (otherwise)      |                                                                   |
+---------------+-------------------------+-------------------------------------------------------------------+

Layers:

-   counts: raw counts
-   data: normalised counts
-   scale.data: scaled normalised counts of variable features

<br /> <strong style='color:red;'> /!\\ IMPORTANT /!\\ </strong> To use all features when calling an integration method with `IntegrateLayers()`: `IntegrateLayers(object, features = Features(object), scale.layer = NULL)`. Does not work for a `SCTAssay`.


### DoIntegrate philosophy

The function `DoIntegrate()` is a very handy way to run multiple integrations
in a single command. Note that:

-   `...` is the place to specify the integration commands to run.
-   integration commands are expected to be **function calls**, i.e. of the form
`FooIntegration()`, `BarIntegration()`.
-   Calls accept method-specific argument (e.g.
`FooIntegration(layers = "data")`)
-   `use.hvg = TRUE` results in using variable features
-   `use.future = TRUE` is useful to run python-based methods (a normal R
session cannot load more than one conda instance, while `future` enables
to launch background sessions, preventing the main on to load any conda
environment.). **It is highly recommended to set it to `FALSE` for R-based
methods**.

Most integration methods can be used without modifying default parameters. In
this vignette, we will change some arguments to meet our needs. Notably, we will
change the number of cores allocated to each method (where possible).

```{r}
ncores <- parallel::detectCores() - 2
```

In this vignette, we are going to use 3 Python-based methods, namely
[BBKNN](https://github.com/Teichlab/bbknn),
[Scanorama](https://github.com/brianhie/scanorama) and
[scANVI](https://github.com/scverse/scvi-tools) from the scvi-tools suite. Let's
make sure they are available straight away:

```{r setup-conda-envs}
# BBKNN
if (! isValid(getCache()$bbknn)) {
  UpdateEnvCache("bbknn")
}
# Scanorama
if (! isValid(getCache()$scanorama)) {
  UpdateEnvCache("scanorama")
}
# scvi-tools
if (! isValid(getCache()$scanvi)) {
  UpdateEnvCache("scanvi")
}
```



<br /> Let's proceed to a few batch-effect corrections:

### Integration part 1)  Using R-based methods


```{r integrate-r-methods}
seu <- DoIntegrate(
  object = seu,
  SeuratIntegrate::HarmonyIntegration(orig = "pca", dims = 1:30,
                                      ncores = ncores),
  CCAIntegration(orig = "pca", dims = 1:30 , new.reduction = "cca.integrated",
                 normalization.method = "SCT"),
  RPCAIntegration(orig = "pca", dims = 1:30, new.reduction = "rpca.integrated",
                  normalization.method = "SCT"),
  use.future = FALSE  # R-based
)
```


### Integration part 2)  Using python-based methods
```{r  integrate-python-methods, warning=FALSE}
seu <- DoIntegrate(
  object = seu,
  bbknnIntegration(orig = "pca", layers = "data", ndims = 30),
  ScanoramaIntegration(orig = "pca", ncores = ncores),
  SeuratIntegrate::scANVIIntegration(groups = seu[[]], groups.name = "Method",
                                     labels.name = "CellType", layers = "counts",
                                     torch.intraop.threads = ncores,
                                     torch.interop.threads = ncores,
                                     max_epochs = 20L),
  use.future = TRUE,  # Python-based
  use.hvg = c(TRUE, TRUE, FALSE)
)
```
> Note: set `max_epochs = 20L` for `scANVIIntegration` is only to save time !
The default number of epochs (400) results in a superior integration.


If we take a look at our Seurat object, we can note that it has been enriched
with many objects:
```{r}
print(seu)
```

```{r}
cat("Graph objects:", paste(Graphs(seu), collapse = ", "), "\n")
cat("Neighbor objects:", paste(Neighbors(seu), collapse = ", "), "\n")
cat("Reduction dimensions:", paste(Reductions(seu), collapse = ", "), "\n")
cat("Assays:", paste(Assays(seu), collapse = ", "), "\n")
```

Now, we will process each integration's output(s) to obtain assessable
representations to score.

## Score integrations

### Process outputs

Several objects can be produced by each integration algorithm, namely a layer in
a new assay (i.e. corrected counts), a dimension reduction (corrected
embedding), or a knn network. Some even produce more than one output (for
instance Scanorama produces corrected counts and a dimension reduction).

The type of output is important to consider, because scoring metrics are not
compatible with all output types. The simplest strategy is to process each
output separately in order to obtain at least a PCA out of it, or even a knn
graph (indispensable to compute clusters). Note that most scores cannot be
computed on knn graphs, hence knn graph outputs (e.g. BBKNN) can only be
evaluated by a reduced set of metrics.

Below is a summary for each output type (bracketed steps are not always necessary):

-   corrected counts: [`ScaleData()`] -> `RunPCA()` -> [`FindNeighbors()` -> `FindOptimalClusters()`]
-   Dimension reduction: [`RunPCA()`] -> [`FindNeighbors()` -> `FindOptimalClusters()`]
-   knn graph:  [`FindOptimalClusters()`]

`RunPCA()` is sometimes ran even on dimension reduction objects (within scoring
functions) because some scores require a variance associated with each dimension.

Let's process all the outputs. Here, we will go through all the steps for a more
exhaustive demonstration. However, it is to be noted that skipping the final
step `FindOptimalClusters()` makes the neighbour graph computation step
(`FindNeighbors()`) unnecessary. In such a case however, one will forgo two
scoring metrics, namely ARI and NMI.

Here, we will use `SymmetrizeKnn()` between `FindNeighbors()` and
`FindOptimalClusters()` because `return.neighbor = TRUE`. This is
useful to keep the distances between cells in the `KNN` graph rather than what
`FindNeighbors()` does by default, which is converting the `KNN` graph to an
adjacency matrix with 0/1s and to a `SNN` network with values bounded between 0
and 1. This not compulsory, but this is used to stay in line with BBKNN's output.
To prevent the community detection algorithm to output a high fraction of
singletons, we "*symmetrize*" the matrix which makes the graph "undirected".
```{r, warning=FALSE}
# corrected counts outputs
DefaultAssay(seu) <- "scanorama.reconstructed"
seu <- ScaleData(seu)
seu <- RunPCA(seu, npcs = 50L, reduction.name = "pca.scanorama_counts")
seu <- FindNeighbors(seu, reduction = "pca.scanorama_counts", dims = 1:30, return.neighbor = TRUE,
                     graph.name = "knn.scanorama_counts")
seu <- SymmetrizeKnn(seu, graph.name = "knn.scanorama_counts")
seu <- FindOptimalClusters(seu, graph.name = "knn.scanorama_counts_symmetric",
                           cluster.name = "scanorama_counts_{cell.var}_{metric}",
                           cell.var = cell.var,
                           optimisation.metric = c("nmi", "ari")) # default, compute both


# dimension reduction outputs
DefaultAssay(seu) <- "SCT"
seu <- FindNeighbors(seu, reduction = "pca", dims = 1:30, k.param = 20L,
                     return.neighbor = TRUE, graph.name = "knn.unintegrated")
seu <- SymmetrizeKnn(seu, graph.name = "knn.unintegrated")
seu <- FindOptimalClusters(seu, graph.name = "knn.unintegrated_symmetric",
                           cluster.name = "unintegrated_{cell.var}_{metric}",
                           cell.var = cell.var)

seu <- FindNeighbors(seu, reduction = "integrated.scanorama", dims = 1:30,
                     return.neighbor = TRUE, graph.name = "knn.scanorama_reduction")
seu <- SymmetrizeKnn(seu, graph.name = "knn.scanorama_reduction")
seu <- FindOptimalClusters(seu, graph.name = "knn.scanorama_reduction_symmetric",
                           cluster.name = "scanorama_reduction_{cell.var}_{metric}",
                           cell.var = cell.var)

seu <- FindNeighbors(seu, reduction = "harmony", dims = 1:30,
                     return.neighbor = TRUE, graph.name = "knn.harmony")
seu <- SymmetrizeKnn(seu, graph.name = "knn.harmony")
seu <- FindOptimalClusters(seu, graph.name = "knn.harmony_symmetric", cell.var = cell.var,
                           cluster.name = "harmony_{cell.var}_{metric}")

seu <- FindNeighbors(seu, reduction = "cca.integrated", dims = 1:30,
                     return.neighbor = TRUE, graph.name = "knn.cca")
seu <- SymmetrizeKnn(seu, graph.name = "knn.cca")
seu <- FindOptimalClusters(seu, graph.name = "knn.cca_symmetric", cell.var = cell.var,
                           cluster.name = "cca_{cell.var}_{metric}")

seu <- FindNeighbors(seu, reduction = "rpca.integrated", dims = 1:30,
                     return.neighbor = TRUE, graph.name = "knn.rpca")
seu <- SymmetrizeKnn(seu, graph.name = "knn.rpca")
seu <- FindOptimalClusters(seu, graph.name = "knn.rpca_symmetric", cell.var = cell.var,
                           cluster.name = "rpca_{cell.var}_{metric}")

seu <- FindNeighbors(seu, reduction = "integrated.scANVI",
                     return.neighbor = TRUE, graph.name = "knn.scanvi")
seu <- SymmetrizeKnn(seu, graph.name = "knn.scanvi")
seu <- FindOptimalClusters(seu, graph.name = "knn.scanvi_symmetric", cell.var = cell.var,
                           cluster.name = "scanvi_{cell.var}_{metric}")


# graph outputs
seu <- SymmetrizeKnn(seu, graph.name = "bbknn_ridge.residuals_distances")
seu <- FindOptimalClusters(seu, graph.name = "bbknn_ridge.residuals_distances_symmetric",
                           cell.var = cell.var, cluster.name = "bbknn_{cell.var}_{metric}")
```
> *Note*: Instead of sticking with the default
`FindNeighbors(return.neighbors = FALSE)` at [the beginning](#standard-worflow),
we could have switched it to `TRUE` right away, process the `KNN` graph with
`SymmetrizeKnn()` and use it for subsequent steps (umap, clustering, etc.)

`FindOptimalClusters()` adds metadata columns with the clustering results that
maximized a metric (NMI or ARI) to the Seurat object:

```{r}
rmarkdown::paged_table(seu[[]][1:10, grep("CellType_[arinm]{3}$", colnames(seu[[]]))])
```


### Add scores to the Seurat object

Now that we have computed objects ready to be scored, we will proceed with
assessing each integration output. For this step, one can either use
`Score[score_name]()` and save scores in separate variables or use
`AddScore[score_name]()` to directly store scores within the Seurat object. The
latter is far more convenient and allows to compare scores graphically. This is
the strategy we are going  to adopt here.

Last but not least, a cell-type variable is used in most scores, hence it is
highly recommended to have an estimate of each cell's type (or produce such an
estimate) stored in the Seurat object as a column in the metadata.


> Note that if one doesn't have a variable with cell-labels (used as ground
truth in multiple scores) he will have to do without several scores.
Alternatively, one can use automatic cell annotation algorithms (e.g. the
[Azimuth package](https://github.com/satijalab/azimuth)). One can also have
multiple cell label variables (e.g. Azimuth typically returns as many cell
label variables as levels of annotations contained in the reference). Scores
requiring cell type annotations always accept multiple column names.

> There is a risk of confusion between cell annotations when using automatic
cell annotation tools. Furthermore, in the case of using Azimuth to annotate
cells, there is a specific risk of biasing results by favouring RPCA integration
because Azimuth uses RPCA to integrate the query dataset onto the reference.

First of all, let's organise outputs in lists:

```{r}
reductions <- list(
  unintegrated = "pca",
  scanorama_counts = "pca.scanorama_counts",
  scanorama_reduction = "integrated.scanorama",
  harmony = "harmony",
  cca = "cca.integrated",
  rpca = "rpca.integrated",
  scanvi = "integrated.scANVI",
  bbknn = NULL
)

graphs <- list(
  unintegrated = "knn.unintegrated_symmetric",
  scanorama_counts = "knn.scanorama_counts_symmetric",
  scanorama_reduction = "knn.scanorama_reduction_symmetric",
  harmony = "knn.harmony_symmetric",
  cca = "knn.cca_symmetric",
  rpca = "knn.rpca_symmetric",
  scanvi = "knn.scanvi_symmetric",
  bbknn = "bbknn_ridge.residuals_distances_symmetric"
)

integrations <- names(reductions)
```

Let's finalise our preparations: :

-   Make sure you have the [kBET package](https://github.com/theislab/kBET)
installed to run `ScoreKBET()`
-   To benefit from a faster implementation of the lisi algorithm, install the
[lisi package](https://github.com/immunogenomics/LISI)
-   To run `ScoreRegressPC.CellCycle()` (score of cell cycle conservation),
run `CellCycleScoringPerBatch()` beforehand (next chunk)

```{r, warning=FALSE}
seu <- CellCycleScoringPerBatch(seu, batch.var = batch.var,
                                s.features = cc.genes$s.genes,
                                g2m.features = cc.genes$g2m.genes)
```


Let's now loop through the integration outputs:

```{r, warning=FALSE, results='hide'}
for (integ in integrations) {
  reduc <- reductions[[integ]]
  graph <- graphs[[integ]]
  clust.var.ari <- paste(integ, cell.var, "ari", sep = "_") # produced by `FindOptimalClusters()`
  clust.var.nmi <- paste(integ, cell.var, "nmi", sep = "_") # produced by `FindOptimalClusters()`
  if (! is.null(reduc)) {   # all TRUE except bbknn
    seu <- AddScoreASW(seu, integration = integ, cell.var = cell.var,
                       what = reduc, dist.package = "distances")
    seu <- AddScoreASWBatch(seu, integration = integ, batch.var = batch.var,
                            cell.var = cell.var, what = reduc,
                            dist.package = "distances")
    seu <- AddScoreDensityPC(seu, integration = integ, batch.var = batch.var,
                             reduction = reduc)
    seu <- AddScoreRegressPC(seu, integration = integ, batch.var = batch.var,
                             reduction = reduc)
    seu <- AddScoreRegressPC.CellCycle(seu, integration = integ,
                                       batch.var = batch.var, what = reduc,
                                       compute.cc = FALSE) # because CellCycleScoringPerBatch was ran before
  }
  seu <- AddScoreARI(seu, integration = integ, cell.var = cell.var,
                     clust.var = clust.var.ari)
  seu <- AddScoreNMI(seu, integration = integ, cell.var = cell.var,
                     clust.var = clust.var.nmi)
  seu <- AddScoreConnectivity(seu, integration = integ, graph.name = graph,
                              cell.var = cell.var)
  seu <- AddScoreKBET(seu, integration = integ, batch.var = batch.var,
                      cell.var = cell.var, what = reduc %||% sub("_symmetric$", "", graph),
                      graph.type = "distances", verbose = FALSE)
  seu <- AddScoreLISI(seu, integration = integ, batch.var = batch.var,
                      cell.var = cell.var, reduction = reduc,
                      graph.name = if (is.null(reduc)) sub("_symmetric$", "", graph) else NULL,
                      graph.type = "distances", ncores = ncores)
  
}
```

Now that we have computed multiple scores, we can look at them using
`IntegrationScores()`:
```{r}
 rmarkdown::paged_table(IntegrationScores(seu))
```

Note that those scores are raw. Let's scale them (and make them comparable):

```{r}
seu <- ScaleScores(seu)
```

Once again, we can print them:
```{r}
rmarkdown::paged_table(IntegrationScores(seu, scaled = TRUE))
```
To readily compare the integrations, let's plot the scores:
```{r}
PlotScores(seu)
```

One might notice a difference in scale for some of the scores, if comparing the
plot with the table just before. This is the case for the PCA density score for
instance.

```{r}
print(sort(IntegrationScores(seu, scaled = TRUE)$PCA.density))
```

Indeed, `PlotScores()` rescales the scores using min-max normalisation by
default (`rescale = TRUE`). One might chose to disable it:

```{r}
PlotScores(seu, rescale = FALSE)
```

We notice that PCA based methods output very low scores in this case. Since they
cannot be computed on knn graphs, scores are biased in favour of BBKNN. You can
exclude some scores (and recompute overall scores on the fly)
```{r}
PlotScores(seu, rescale = FALSE, exclude.score = c("pca.density", "pca.regression"))
```


You can chose a different type of plot (`radar` or `lollipop`):
```{r, fig.height=8, fig.width=12}
library(ggplot2)
PlotScores(seu, plot.type = "radar") +
  # reduce overlap between axis names and figures 
  theme(legend.position = "top", panel.spacing = unit(3, "cm"),
        plot.margin = margin(r = 3, l = 3, unit = "cm"),
        axis.text.x = element_text(size = 10))
```

In this last plot, we also exclude the non-integrated dataset. Since the
`rescale` argument is true by default, the scores are rescaled without the
excluded integration's scores.
```{r, fig.width=9, fig.height=6}
  PlotScores(seu, plot.type = "lollipop",
             exclude.integration = "unintegrated")
```

We want to compare the UMAP embeddings. For this, we first compute the dimension
reductions:
```{r}
for (integ in integrations) {
  reduc <- reductions[[integ]]
  if (! is.null(reduc)) { # all except BBKNN
    seu <- RunUMAP(seu, dims = 1:min(30, ncol(seu[[reduc]])), reduction = reduc,
                   reduction.name = paste0(integ, ".umap"))
  }
}
```

As BBKNN's output is a graph, we need to use the umap Python package: 
```{r , results='hide'}
library(future)
if (! reticulate::condaenv_exists('umap_0.5.4')) {
  reticulate::conda_create('umap_0.5.4', packages = 'umap=0.5.4')
}
plan(multisession)
seu %<-% {
  reticulate::use_condaenv('umap_0.5.4')
  RunUMAP(seu, graph = "bbknn_ridge.residuals_connectivities", umap.method = "umap-learn",
          n.epochs = 200L, reduction.name = "bbknn.umap") }
seu
plan(sequential)
```


```{r, fig.width=10, fig.height=8}
library(ggplot2)
plot_list <- sapply(integrations, function(integ) {
  DimPlot(seu, reduction = paste0(integ, ".umap"), group.by = batch.var) +
    ggtitle(integ) +
    theme(axis.title = element_blank())
}, simplify = FALSE)

patchwork::wrap_plots(plot_list, guides = "collect")
```

```{r, fig.width=10, fig.height=8}
library(ggplot2)
plot_list <- sapply(integrations, function(integ) {
  DimPlot(seu, reduction = paste0(integ, ".umap"), group.by = cell.var) +
    ggtitle(integ) +
    theme(axis.title = element_blank())
}, simplify = FALSE)

patchwork::wrap_plots(plot_list, guides = "collect")
```

There several observations to be made here, that require further explanations:

-   some cells seem to be assigned a wrong label in `CellType`, highlighting
the importance of having cell annotations of sufficient quality to be considered
suitable as ground truth (which is actually not the case here)
-   All scaled PCA regression scores are set to zero. This is because the
unintegrated dataset has the lowest raw PCA regression score. It is very likely
the consequence of the `SCT` normalisation, which is much more efficient at
masking batch-specific differences than the classical `LogNorm`. Thus, the
inter-batch differences are not driving the principal components.
-  `ScaleScores()` produce scores that can have different scales (as long as
`rescale = FALSE`) . Thus, min-max rescaling is used by default in
`PlotScores()`, to balance each score's contribution to the overall scores. This
is especially suited for comparing a large number of integrations. However, it
has some drawbacks: it can heavily distort the scale of scores when their
maximum or minimum are far from 1 or 0 respectively (e.g. all cLISI scores are
above 0.9). Hence, the **final decision** on whether to use min-max rescaling
**is left to the user's discretion**.
